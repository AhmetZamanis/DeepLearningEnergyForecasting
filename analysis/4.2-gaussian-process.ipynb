{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62bd4214-e9ea-4058-bd70-e97ab19ac65c",
   "metadata": {},
   "source": [
    "This notebook uses the [GPyTorch package](https://github.com/cornellius-gp/gpytorch) to apply Gaussian Process regression to the multi-step energy consumption forecasting problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08149750-e8ab-40e7-bbe8-fc184e72c1be",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a418d22-4023-4f4d-93bc-8280ed0a961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gpytorch\n",
    "import torch\n",
    "\n",
    "from gpytorch.kernels import ScaleKernel, LinearKernel, PeriodicKernel, AdditiveKernel, ProductKernel\n",
    "from gpytorch.priors import NormalPrior\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d2852-8794-427f-9d0a-d240d55cb141",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1923"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe418ee7-9e5d-44f4-9cc3-0eb0f48ba691",
   "metadata": {},
   "source": [
    "Using the torch.float32 datatype seems to cause numerical stability issues, especially with variational inference & zero values. Using float64 and disabling mixed precision fixes them but slows down training quite a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a6d62-f05e-4689-afde-6edd36dac68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Torch settings\n",
    "torch.set_default_dtype(torch.float64)\n",
    "#torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd871b-5f69-4cab-8583-d5a58dad00fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcbe585-0804-4a60-a97e-2d3a957c0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./OutputData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e98cf-95b1-477f-966d-83d1d5064616",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_dir + \"full_data.csv\")\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"], format = \"%d:%m:%Y:%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac72c8-f4b5-4cce-bcc3-439640f338dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop generation columns\n",
    "gen_cols = df.columns.values[2:].tolist()\n",
    "df = df.drop(gen_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d9ff2-4bf8-41ca-a272-a601006d0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ae553b-5c8b-49c7-a486-6c5185bf111e",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a92aba0-eac9-4eb3-a5f5-2bc7ebbd3c28",
   "metadata": {},
   "source": [
    "# Using the consumption lag as a predictor would require the model to be applied autoregressively for multi-horizon forecasts.\n",
    "df[\"consumption_lag2\"] = df[\"consumption_MWh\"].shift(2)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196c616-a1ce-4339-8ab7-c83406f0dd4e",
   "metadata": {},
   "source": [
    "We do not need to cyclical encode seasonal features, as we will apply periodic kernels to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac49c4-dc2f-4768-8449-6b723c6127bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time columns\n",
    "\n",
    "# Trend\n",
    "df[\"trend\"] = df.index.values\n",
    "\n",
    "# Hour of day\n",
    "df[\"hour\"] = df.time.dt.hour + 1\n",
    "\n",
    "# Day of week\n",
    "df[\"dayofweek\"] = df.time.dt.dayofweek + 1\n",
    "\n",
    "# Month\n",
    "df[\"month\"] = df.time.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82996eeb-c9ae-4f26-92ca-752ed658a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ff6f2-7ef9-4bde-9082-b24a1b1d2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features & target\n",
    "X = df.drop([\"time\", \"consumption_MWh\",], axis = 1).values\n",
    "y = df[\"consumption_MWh\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d5116-dcf6-4035-a797-6ca7152addfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "first_t = df[df[\"time\"] == '2022-10-18 16:00:00'].index[0] # First prediction point\n",
    "X_train_raw, X_test_raw = X[:first_t, :], X[first_t:, :]\n",
    "y_train_raw, y_test_raw = y[:first_t], y[first_t:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6ebe0-cf5f-4839-a464-f410efd0259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature & target scaling (0-1), tensor conversion\n",
    "scaler = MinMaxScaler()\n",
    "X_train = torch.tensor(scaler.fit_transform(X_train_raw))\n",
    "X_test = torch.tensor(scaler.transform(X_test_raw))\n",
    "\n",
    "scaler_target = MinMaxScaler()\n",
    "y_train = torch.tensor(scaler_target.fit_transform(y_train_raw.reshape(-1, 1))).squeeze(-1)\n",
    "y_test = torch.tensor(scaler_target.transform(y_test_raw.reshape(-1, 1))).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11356245-6bc8-4084-addc-71ecf6038146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset training data to fit into GPU memory\n",
    "train_size = int(24 * 365 * 1)\n",
    "X_train = X_train[-train_size:, :]\n",
    "y_train = y_train[-train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce54873e-f8b5-4586-9f2e-22ebee567fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85695f2d-70d5-4c31-a16e-61dddb183f55",
   "metadata": {},
   "source": [
    "## Model & wrapper definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5727d65f-006f-418b-bcb0-964639f8cd1a",
   "metadata": {},
   "source": [
    "Summary of training strategies tried:\n",
    "- ExactGP can only be trained with unbatched gradient descent. The full training data does not fit into GPU memory. Training with the last ~9k observations in the training set works quite well for predicting the first few days of the testing set. Predictions move away from the series level as the distance from the training set increases, which is expected. Could be solved by rolling online updates of training data.\n",
    "- SVGP can be trained with with batched gradient descent. Only fits into GPU memory with few inducing points, because inducing points are unbatched, kind of defeating the purpose of using SGD. Does not fit & converge properly, likely due to reliance on inducing points.\n",
    "- VNNGP supports batching the inducing points, essentially using the entire data as both training data & inducing points. The initial computing of the k-nearest neighbor structure (when the model is created, not trained) is slow, but can be made faster by installing the faiss package. The entire data can be used as inducing points, but training loss is inexplicably high compared to SVGP (200k to 2k in 15-20 epochs), and predictions fluctuate around zero seemingly randomly. Also, constant warnings for negative variances are raised. I don't know why exactly this training approach fails to learn while the exact method does quite well. My theory is that the nearest neighbor structure somehow does not work with ordered time series data.\n",
    "\n",
    "Based on all this, we continue with the ExactGP method, subsetting the training data. All of the strategies above were tried with the same covariance kernel choices: A linear kernel applied to the time series index (trend dummy), and a separate periodic kernel applied to each seasonal feature (hour, day of week, month of year), all combined additively.\n",
    "- Applying all kernels to just the time index failed to learn the seasonality properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd89e6-4c21-452f-96ca-e06f9db81c34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ExactGP model class\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "\n",
    "    def __init__(self, X_train, y_train, likelihood):\n",
    "        super().__init__(X_train, y_train, likelihood)\n",
    "\n",
    "        # Create mean module\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "\n",
    "        # Create covariance module\n",
    "        self.covar_module = AdditiveKernel(\n",
    "            LinearKernel(active_dims = 0), # Linear trend\n",
    "            ScaleKernel(PeriodicKernel(\n",
    "                active_dims = (1),\n",
    "                period_length_prior = NormalPrior(24, 1) # Hourly seasonality\n",
    "            )),\n",
    "            ScaleKernel(PeriodicKernel(\n",
    "                active_dims = (2),\n",
    "                period_length_prior = NormalPrior(7, 1) # Day of week seasonality\n",
    "            )),\n",
    "            ScaleKernel(PeriodicKernel(\n",
    "                active_dims = (3),\n",
    "                period_length_prior = NormalPrior(12, 1) # Year of month seasonality\n",
    "            )),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74554259-6cfc-4e2f-b1ef-c62fc4c06d86",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ExactGP wrapper class (unbatched gradient descent)\n",
    "class ExactGP:\n",
    "    \n",
    "    def __init__(self, model, likelihood, cuda = True):\n",
    "        self.cuda = cuda\n",
    "\n",
    "        # Put model & likelihood on GPU if cuda is enabled\n",
    "        if cuda:\n",
    "            self.model = model.cuda()\n",
    "            self.likelihood = likelihood.cuda()\n",
    "        else:\n",
    "            self.model = model\n",
    "            self.likelihood = likelihood\n",
    "            \n",
    "    # Training method\n",
    "    def train(self, X_train, y_train, max_epochs, learning_rate = 0.01, early_stop = 5, early_stop_tol = 1e-3):\n",
    "\n",
    "        # Put data on GPU if cuda is enabled\n",
    "        if self.cuda:\n",
    "            X_train = X_train.cuda()\n",
    "            y_train = y_train.cuda()\n",
    "\n",
    "        # Put models into training mode\n",
    "        self.model.train()\n",
    "        self.likelihood.train()\n",
    "\n",
    "        # Create Adam optimizer with model parameters\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr = learning_rate)\n",
    "\n",
    "        # Create marginal log likelihood loss\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(max_epochs):\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get outputs from model\n",
    "            output = self.model(X_train)\n",
    "\n",
    "            # Calculate loss and perform backpropagation\n",
    "            loss = -mll(output, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Get loss & noise values to be printed\n",
    "            loss_scalar = loss.item()\n",
    "            noise = self.model.likelihood.noise.item()\n",
    "            \n",
    "            # Initialize best loss & rounds with no improvement if first epoch\n",
    "            if epoch == 0:\n",
    "                self._best_epoch = epoch\n",
    "                self._best_loss = loss_scalar\n",
    "                self._epochs_no_improvement = 0\n",
    "                self._best_state_dict = self.model.state_dict()\n",
    "\n",
    "            # Record an epoch with no improvement\n",
    "            if self._best_loss < loss_scalar - early_stop_tol:\n",
    "                self._epochs_no_improvement += 1\n",
    "\n",
    "            # Record an improvement in the loss\n",
    "            if self._best_loss > loss_scalar + early_stop_tol:\n",
    "                self.best_epoch = epoch\n",
    "                self._best_loss = loss_scalar\n",
    "                self._epochs_no_improvement = 0\n",
    "                self._best_state_dict = self.model.state_dict()\n",
    "\n",
    "            # Print epoch summary\n",
    "            print(f\"Epoch: {epoch+1}/{max_epochs}, Loss: {loss_scalar}, Noise: {noise}, Best loss: {self._best_loss}\")\n",
    "\n",
    "            # Early stop if necessary\n",
    "            if self._epochs_no_improvement >= early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Load best checkpoint after training \n",
    "        self.model.load_state_dict(self._best_state_dict)\n",
    "\n",
    "        # Delete unneeded tensors\n",
    "        del X_train, y_train, optimizer, mll, output, loss, loss_scalar, noise\n",
    "\n",
    "        # Clear GPU memory \n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "    # Predict method\n",
    "    def predict(self, X_test, cpu = True, fast_preds = False):\n",
    "\n",
    "        # Test data to GPU, if cuda enabled\n",
    "        if self.cuda:\n",
    "            X_test = X_test.cuda()\n",
    "\n",
    "        # Activate eval mode\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        # Make predictions without gradient calculation\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var(state = fast_preds):\n",
    "\n",
    "            # Returns the model posterior distribution over functions: p(f*|X_test, X_train, y_train)\n",
    "            # Noise is not yet added to the functions\n",
    "            f_posterior = self.model(X_test)\n",
    "\n",
    "            # Returns the predictive posterior distribution: p(y*|X_test, X_train, y_train)\n",
    "            # Noise is added to the functions\n",
    "            y_posterior = self.likelihood(f_posterior)\n",
    "\n",
    "            # Get posterior predictive mean & prediction intervals\n",
    "            # By default, 2 standard deviations around the mean\n",
    "            y_mean = y_posterior.mean\n",
    "            y_lower, y_upper = y_posterior.confidence_region()\n",
    "\n",
    "        # Return predictions to CPU if desired\n",
    "        # Could be advantageous to stack the predictions into one array\n",
    "        if cpu:\n",
    "            y_mean = y_mean.cpu()\n",
    "            y_lower = y_lower.cpu()\n",
    "            y_upper = y_upper.cpu()\n",
    "\n",
    "        # Delete unneeded tensors\n",
    "        del X_test, f_posterior, y_posterior\n",
    "\n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return y_mean, y_lower, y_upper\n",
    "\n",
    "    # Method to update model training data (kernel hyperparameters unchanged, no additional training performed)\n",
    "    # When done repeatedly, GPU memory fills up, couldn't determine exact cause or find a fix.\n",
    "    def update_train(self, X_update, y_update):\n",
    "        \n",
    "        # Put tensors on GPU if cuda is enabled\n",
    "        if self.cuda:\n",
    "            X_update = X_update.cuda()\n",
    "            y_update = y_update.cuda()\n",
    "\n",
    "        # Update model training data\n",
    "        self.model = self.model.get_fantasy_model(X_update, y_update)\n",
    "\n",
    "        # Delete unneeded tensors\n",
    "        del X_update, y_update\n",
    "\n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Method to save model state dict\n",
    "    def save(self, save_dir):\n",
    "        torch.save(self.model.state_dict(), save_dir)\n",
    "\n",
    "    # Method to load model parameters from saved state dict\n",
    "    def load(self, state_dict):\n",
    "        self.model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c21642-37e4-4a0f-a203-f945721ebeec",
   "metadata": {},
   "source": [
    "## Model training & testing without online updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f8086-da6e-46a7-9aa6-60a161aaed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create likelihood, model, wrapper\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(X_train, y_train, likelihood)\n",
    "trainer = ExactGP(model, likelihood)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab4abd94-e41c-445e-9f53-c44f7cba1351",
   "metadata": {},
   "source": [
    "# Perform training\n",
    "trainer.train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    max_epochs = 100,\n",
    "    learning_rate = 0.1,\n",
    "    early_stop = 10,\n",
    "    early_stop_tol = 1e-3)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d2237cb-eabb-448c-ab75-938ea3012890",
   "metadata": {},
   "source": [
    "# Save model state\n",
    "model_name = \"ExactGP1.pth\"\n",
    "trainer.save(output_dir + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f86d041-c746-40ac-b735-beb033ae9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model state\n",
    "model_name = \"ExactGP1.pth\"\n",
    "state_dict = torch.load(output_dir + model_name)\n",
    "trainer.load(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b01f07-8230-4ce0-8025-e688a3e81784",
   "metadata": {},
   "source": [
    "Inference is also GPU intensive with a large dataset, so we subset the testing data to the training data size.\n",
    "\\\n",
    "Wouldn't be a problem in model deployment, as we would keep the forecast horizon short, and load the trained model parameters from a .pth file without performing training in the same run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a3b36a-27f8-4139-9d8c-a4de2007a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "preds_mean, preds_upper, preds_lower = trainer.predict(X_test[:train_size, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95f3039-4dac-4282-9937-cc98429420f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtransform predictions to original scale\n",
    "preds_mean = scaler_target.inverse_transform(preds_mean.numpy().reshape(-1, 1)).squeeze()\n",
    "preds_upper = scaler_target.inverse_transform(preds_upper.numpy().reshape(-1, 1)).squeeze()\n",
    "preds_lower = scaler_target.inverse_transform(preds_lower.numpy().reshape(-1, 1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f129f253-74bb-4d2c-b337-8ef16ae40ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted vs. actual, first N days of testing data\n",
    "plot_hours = 24 * 5\n",
    "\n",
    "_ = sns.lineplot(\n",
    "    x = X_test[:plot_hours, 0], \n",
    "    y = y_test_raw[:plot_hours], \n",
    "    label = \"Actual\")\n",
    "\n",
    "_ = sns.lineplot(\n",
    "    x = X_test[:plot_hours, 0], \n",
    "    y = preds_mean[:plot_hours], \n",
    "    label = \"Predicted, 2sd interval\")\n",
    "\n",
    "_ = plt.fill_between(\n",
    "    X_test[:plot_hours, 0], \n",
    "    preds_lower[:plot_hours], \n",
    "    preds_upper[:plot_hours], \n",
    "    color = \"orange\", \n",
    "    alpha = 0.4)\n",
    "\n",
    "_ = plt.title(f\"Exact GP predictions, no online training data updates\\nForecast horizon: {plot_hours} hours\\nTraining data length: {len(y_train)} hours\")\n",
    "_ = plt.xlabel(\"Scaled time index, 1 = end of train set\")\n",
    "_ = plt.ylabel(\"Hourly energy consumption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9157ce7-a4ca-4b22-9735-a232410b3e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted vs. actual, entire test set\n",
    "_ = sns.lineplot(\n",
    "    x = X_test[:train_size, 0], \n",
    "    y = y_test_raw[:train_size], \n",
    "    label = \"Actual\")\n",
    "\n",
    "_ = sns.lineplot(\n",
    "    x = X_test[:train_size, 0], \n",
    "    y = preds_mean, \n",
    "    label = \"Predicted, 2sd interval\")\n",
    "\n",
    "_ = plt.fill_between(\n",
    "    X_test[:train_size, 0], \n",
    "    preds_lower, \n",
    "    preds_upper,\n",
    "    color = \"orange\",\n",
    "    alpha = 0.4)\n",
    "\n",
    "_ = plt.title(f\"Exact GP predictions, no online training data updates\\nForecast horizon: {train_size} hours\\nTraining data length: {len(y_train)} hours\")\n",
    "_ = plt.xlabel(\"Scaled time index, 1 = end of train set\")\n",
    "_ = plt.ylabel(\"Hourly energy consumption\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82300dd-a542-4826-b9a1-b22cd4de0ef4",
   "metadata": {},
   "source": [
    "The initial predictions for the first few days are pretty good, and the kernels seem to be suitable for learning the seasonality of the data.\n",
    "\\\n",
    "\\\n",
    "As the time index moves away from the last training point, the forecasts considerably deviate from the level of the data, but that is very natural with an effective forecast horizon of ~9k points. The lack of a past target covariate may be preventing the model from reacting to short-term cyclicality, as the LSTM and Transformer did not have this problem overall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a84582-096d-49dc-9ae2-32165b646a80",
   "metadata": {},
   "source": [
    "## Model testing with online updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b064d59-c047-4531-9b3c-f2d2bbbc1df0",
   "metadata": {},
   "source": [
    "It's possible to update the GP's training data with new data, extending its relevance to longer forecast horizons. Hyperparameters are not updated, no additional training or optimization takes place. This mirrors the usage of input sequences during prediction time in time series forecasting NN models. We condition the model on new data, essentially providing it with new context.\n",
    "\\\n",
    "\\\n",
    "After each prediction, we will extend the training data in rolling fashion, with the newly predicted datapoints and the following 24 hours. This way, each new prediction starts from 16:00. \n",
    "\\\n",
    "\\\n",
    "Issue: GPU memory fills up as the model training data is updated. The loop still returns the predictions made before the GPU runs out of memory, so no need to rerun with a subset of the data. Again, this wouldn't be a problem in deployment, as we would likely do a single model update + prediction in one run, with a short horizon. We can also retrain the model with training data shifted forward in time, keeping the training data size constant as we extend it into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf01c5-706c-44a3-93fc-bbb5147eabbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation parameters that match the sequence2sequence testing scheme\n",
    "horizon = 32 # Forecast horizon\n",
    "stride = 24 # Number of timesteps between each prediction point\n",
    "first_t = df[df[\"time\"] == '2022-10-18 16:00:00'].index[0] # First prediction point\n",
    "last_t = df.loc[(df.time.dt.hour == 16) & (df.index + horizon - 1 <= df.index.values[-1])].index.values[-2] # Last prediction point\n",
    "n_windows = len(range(first_t + 1, last_t + 1, stride)) # N. of eval windows after first"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8ffd96c-26bd-4da7-ac3e-8d47a14fedc4",
   "metadata": {},
   "source": [
    "# Limit the number of testing windows\n",
    "n_windows = int(n_windows * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fcf4b-e90a-459b-b44e-67388f8eea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence2sequence testing loop\n",
    "\n",
    "# Initialize predictions with first forecast horizon\n",
    "preds_mean, preds_upper, preds_lower = trainer.predict(X_test[:horizon, :])\n",
    "\n",
    "# Initialize predictions' time step indices\n",
    "preds_indices = X_test[:horizon, 0]\n",
    "\n",
    "# Update model training points with predicted points & steps before next prediction point\n",
    "train_end = horizon + stride\n",
    "trainer.update_train(\n",
    "    X_test[:train_end, :],\n",
    "    y_test[:train_end]\n",
    ")\n",
    "\n",
    "# Repeat for remaining evaluation windows\n",
    "for window in range(0, n_windows):\n",
    "\n",
    "    # Make predictions\n",
    "    mean, upper, lower = trainer.predict(\n",
    "        X_test[train_end:(train_end + horizon), :]\n",
    "    )\n",
    "\n",
    "    # Get predictions' time step indices\n",
    "    indices = X_test[train_end:(train_end + horizon), 0]\n",
    "\n",
    "    # Concatenate predictions & indices\n",
    "    preds_mean = torch.cat((preds_mean, mean))\n",
    "    preds_upper = torch.cat((preds_upper, upper))\n",
    "    preds_lower = torch.cat((preds_lower, lower))\n",
    "    preds_indices = torch.cat((preds_indices, indices))\n",
    "\n",
    "    # Update model training points with predicted points & steps before next prediction point\n",
    "    new_train_end = train_end + horizon + stride\n",
    "    trainer.update_train(\n",
    "        X_test[train_end:new_train_end, :],\n",
    "        y_test[train_end:new_train_end]\n",
    "    )\n",
    "    train_end = new_train_end\n",
    "\n",
    "    # Delete unused tensors & clear GPU memory\n",
    "    del mean, upper, lower, indices\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3b6a1-59bc-4079-8a57-8006ba920eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e93dbf9-4fe8-46a8-90e4-9a4b976b4eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6789b6-26aa-42b7-be25-7dcf49051b6a",
   "metadata": {},
   "source": [
    "We're able to predict roughly ~2k out of the previously predicted ~9k testing observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c340d1-1d90-483b-ba19-5f57674bf297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtransform predictions to original scale\n",
    "preds_mean = scaler_target.inverse_transform(preds_mean.numpy().reshape(-1, 1)).squeeze()\n",
    "preds_upper = scaler_target.inverse_transform(preds_upper.numpy().reshape(-1, 1)).squeeze()\n",
    "preds_lower = scaler_target.inverse_transform(preds_lower.numpy().reshape(-1, 1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d49a9-6589-4055-86cd-0325490f5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted vs. actual, entire test set\n",
    "last_pred_idx = np.where(X_test[:, 0].numpy() == preds_indices[-1].numpy())[0][0]\n",
    "\n",
    "_ = sns.lineplot(\n",
    "    x = X_test[:last_pred_idx, 0], \n",
    "    y = y_test_raw[:last_pred_idx], \n",
    "    label = \"Actual\")\n",
    "\n",
    "_ = sns.lineplot(\n",
    "    x = preds_indices, \n",
    "    y = preds_mean, \n",
    "    label = \"Predicted, 2sd interval\")\n",
    "\n",
    "_ = plt.fill_between(\n",
    "    preds_indices, \n",
    "    preds_lower, \n",
    "    preds_upper, \n",
    "    color = \"orange\", \n",
    "    alpha = 0.4)\n",
    "\n",
    "_ = plt.title(f\"Exact GP predictions with online training data updates\\nForecast horizon: {horizon} hours\\nTraining data length: {len(y_train)} hours\")\n",
    "_ = plt.xlabel(\"Scaled time index, 1 = end of train set\")\n",
    "_ = plt.ylabel(\"Hourly energy consumption\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d88db8-38f5-48b5-b217-6b0534d5a2e5",
   "metadata": {},
   "source": [
    "The plot is a little bit misleading, as the predicted values line (orange) is only plotted for the x-axis values where a prediction was made. The gaps, which correspond to the 24-hour stride periods without predictions, are connected with straight lines. Still, we can see the forecasts now continue to match the level of the data quite well as the predictions extend into the future. The forecast intervals also cover the actual values pretty well, which could be all one can ask for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ee520-77a3-4a4f-bcfc-633295274c3d",
   "metadata": {},
   "source": [
    "To sum up, the GP forecasting approach is probably best considered when:\n",
    "- The training time series is not too long, or the near history is sufficient to make good predictions,\n",
    "- The number of covariates is few, and the kernel choices for them are straightforward,\n",
    "- Seasonality is strong & fixed (or there are other strongly predictive & easily available future covariates),\n",
    "- Either a fixed trend is present, or the series level is constant,\n",
    "- Cyclicality is low, or to put it another way, the additional predictive value of the past target values (or additional past covariates) are low,\n",
    "- Online training data updates can be used to extend the model's relevance into the future,\n",
    "- The predictions are not required real-time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
