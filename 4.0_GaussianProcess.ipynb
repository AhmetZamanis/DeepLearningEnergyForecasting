{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62bd4214-e9ea-4058-bd70-e97ab19ac65c",
   "metadata": {},
   "source": [
    "This notebook uses the GPyTorch package to apply Gaussian Process regression to the multi-step energy consumption forecasting problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08149750-e8ab-40e7-bbe8-fc184e72c1be",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a418d22-4023-4f4d-93bc-8280ed0a961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gpytorch\n",
    "import torch\n",
    "\n",
    "\n",
    "from gpytorch.kernels import ScaleKernel, LinearKernel, PeriodicKernel, AdditiveKernel, ProductKernel\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.priors import NormalPrior\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution, NaturalVariationalDistribution\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d2852-8794-427f-9d0a-d240d55cb141",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1923"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe418ee7-9e5d-44f4-9cc3-0eb0f48ba691",
   "metadata": {},
   "source": [
    "Using torch.float32 datatype seems to cause issues with zero values in matrix algebra. Using float64 and disabling mixed precision fixes them but slows down training quite a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a6d62-f05e-4689-afde-6edd36dac68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Torch settings\n",
    "torch.set_default_dtype(torch.float64)\n",
    "#torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd871b-5f69-4cab-8583-d5a58dad00fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcbe585-0804-4a60-a97e-2d3a957c0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./OutputData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e98cf-95b1-477f-966d-83d1d5064616",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_dir + \"full_data.csv\")\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"], format = \"%d:%m:%Y:%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac72c8-f4b5-4cce-bcc3-439640f338dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop generation columns\n",
    "gen_cols = df.columns.values[2:].tolist()\n",
    "df = df.drop(gen_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d9ff2-4bf8-41ca-a272-a601006d0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ae553b-5c8b-49c7-a486-6c5185bf111e",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a92aba0-eac9-4eb3-a5f5-2bc7ebbd3c28",
   "metadata": {},
   "source": [
    "# Using the consumption lag as a predictor would require the model to be applied autoregressively for multi-horizon forecasts.\n",
    "df[\"consumption_lag2\"] = df[\"consumption_MWh\"].shift(2)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196c616-a1ce-4339-8ab7-c83406f0dd4e",
   "metadata": {},
   "source": [
    "We do not need to cyclical encode seasonal features, as we will apply periodic kernels to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac49c4-dc2f-4768-8449-6b723c6127bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time columns\n",
    "\n",
    "# Trend\n",
    "df[\"trend\"] = df.index.values\n",
    "\n",
    "# Hour of day\n",
    "df[\"hour\"] = df.time.dt.hour + 1\n",
    "\n",
    "# Day of week\n",
    "df[\"dayofweek\"] = df.time.dt.dayofweek + 1\n",
    "\n",
    "# Month\n",
    "df[\"month\"] = df.time.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82996eeb-c9ae-4f26-92ca-752ed658a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf01c5-706c-44a3-93fc-bbb5147eabbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation parameters that match the sequence2sequence testing scheme\n",
    "horizon = 32 # Forecast horizon\n",
    "first_t = df[df[\"time\"] == '2022-10-18 16:00:00'].index[0] # First prediction point\n",
    "stride = 24 # Number of timesteps between each prediction point\n",
    "\n",
    "# Dataloader arguments\n",
    "batch_size = 1024\n",
    "shuffle = False\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ff6f2-7ef9-4bde-9082-b24a1b1d2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features & target\n",
    "X = df.drop([\"time\", \"consumption_MWh\",], axis = 1).values\n",
    "y = df[\"consumption_MWh\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d5116-dcf6-4035-a797-6ca7152addfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test = X[:first_t, :], X[first_t:, :]\n",
    "y_train, y_test = y[:first_t], y[first_t:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6ebe0-cf5f-4839-a464-f410efd0259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature & target scaling (0-1), tensor conversion\n",
    "scaler = MinMaxScaler()\n",
    "X_train = torch.tensor(scaler.fit_transform(X_train))\n",
    "X_test = torch.tensor(scaler.transform(X_test))\n",
    "\n",
    "scaler_target = MinMaxScaler()\n",
    "y_train = torch.tensor(scaler_target.fit_transform(y_train.reshape(-1, 1))).squeeze(-1)\n",
    "y_test = torch.tensor(scaler_target.transform(y_test.reshape(-1, 1))).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49be72-1e73-44b9-af61-0574ff55683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor datasets & dataloaders\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, num_workers = num_workers, shuffle = False)\n",
    "\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, num_workers = num_workers, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85695f2d-70d5-4c31-a16e-61dddb183f55",
   "metadata": {},
   "source": [
    "## Model & wrapper definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5727d65f-006f-418b-bcb0-964639f8cd1a",
   "metadata": {},
   "source": [
    "Summary so far:\n",
    "- ExactGP can only be trained with unbatched gradient descent, roughly 10k observations. Does a good job for predicting the first few days of the testing set, declines to zero across time. Would likely be solved by online updates of training data.\n",
    "- VariationalGP can be trained with with batched, stochastic & natural gradient descent. Only fits into GPU memory with few inducting points, because inducting points are unbatched, kind of defeating the purpose of using SGD. Does not fit & converge properly with few inducting points.\n",
    "- VNNGP supports batching the inducting points, essentially using the entire data as both training & inducting points. Give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd89e6-4c21-452f-96ca-e06f9db81c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variational GP model class\n",
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "\n",
    "    def __init__(self, inducing_points, learn_inducting_locations = False):\n",
    "\n",
    "        # Initialize variational parameters\n",
    "        variational_distribution = NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations = learn_inducting_locations)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "\n",
    "        # Initialize mean function\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        #self.mean_module = gpytorch.means.ZeroMean()\n",
    "\n",
    "        # Initialize covariance kernel\n",
    "        self.covariance_module = AdditiveKernel(\n",
    "            LinearKernel(active_dims = 0),\n",
    "            ScaleKernel(PeriodicKernel(\n",
    "                active_dims = (1),\n",
    "                period_length_prior = NormalPrior(24, 1) # Applied to hour feature\n",
    "            )),\n",
    "            ScaleKernel(PeriodicKernel(\n",
    "                active_dims = (2),\n",
    "                period_length_prior = NormalPrior(7, 1) # Applied to day of week feature\n",
    "            )),\n",
    "            ScaleKernel(PeriodicKernel(\n",
    "                active_dims = (3),\n",
    "                period_length_prior = NormalPrior(12, 1) # Applied to month feature\n",
    "            )),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covariance_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74554259-6cfc-4e2f-b1ef-c62fc4c06d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational GP wrapper class\n",
    "class VariationalMinibatchGP:\n",
    "    \n",
    "    def __init__(self, model, likelihood, cuda = True):\n",
    "        self.model = model\n",
    "        self.likelihood = likelihood\n",
    "        self.cuda = cuda\n",
    "\n",
    "    # Training method\n",
    "    def train(self, train_loader, num_data, max_epochs, learning_rate = 0.1, early_stop = 10, early_stop_tol = 1e-4):\n",
    "\n",
    "        # Put model & likelihood on GPU if cuda is enabled\n",
    "        if self.cuda:\n",
    "            self.model = self.model.cuda()\n",
    "            self.likelihood = self.likelihood.cuda()\n",
    "\n",
    "        # Find optimal kernel hyperparameters\n",
    "        self.model.train()\n",
    "        self.likelihood.train()\n",
    "\n",
    "        # Create variational optimizer for natural gradient descent\n",
    "        var_optimizer = gpytorch.optim.NGD(\n",
    "            self.model.variational_parameters(),\n",
    "            num_data = num_data,\n",
    "            lr = learning_rate\n",
    "        )\n",
    "\n",
    "        # Create hyperparameter optimizer with model parameters\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {\"params\": self.model.parameters()},\n",
    "            {\"params\": self.likelihood.parameters()}\n",
    "        ], lr = learning_rate)\n",
    "\n",
    "        # Create loss\n",
    "        mll = gpytorch.mlls.VariationalELBO(self.likelihood, self.model, num_data = num_data)\n",
    "\n",
    "        # N. of batches for epoch loss calculation\n",
    "        num_batches = len(train_loader)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(max_epochs):\n",
    "\n",
    "            # Initialize loss tracking for epoch\n",
    "            total_loss_epoch = 0\n",
    "\n",
    "            # Iterate over batches\n",
    "            minibatch_iter = tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "            for X, y in minibatch_iter:\n",
    "\n",
    "                # Put tensors on cuda if enabled\n",
    "                if self.cuda:\n",
    "                    X = X.cuda()\n",
    "                    y = y.cuda()\n",
    "\n",
    "                # Reset gradients\n",
    "                optimizer.zero_grad()\n",
    "                var_optimizer.zero_grad()\n",
    "    \n",
    "                # Get outputs from model\n",
    "                output = self.model(X)\n",
    "    \n",
    "                # Calculate loss perform backpropagation, adjust weights\n",
    "                batch_loss = -mll(output, y)\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                var_optimizer.step()\n",
    "\n",
    "                # Print batch loss\n",
    "                batch_loss_scalar = batch_loss.item()\n",
    "                minibatch_iter.set_postfix(loss = batch_loss_scalar)\n",
    "    \n",
    "                # Save batch loss\n",
    "                total_loss_epoch += batch_loss_scalar\n",
    "\n",
    "            # Calculate epoch loss\n",
    "            epoch_loss = total_loss_epoch / num_batches\n",
    "        \n",
    "            # Initialize best loss & rounds with no improvement if first epoch\n",
    "            if epoch == 0:\n",
    "                self._best_epoch = epoch\n",
    "                self._best_loss = epoch_loss\n",
    "                self._epochs_no_improvement = 0\n",
    "            \n",
    "            # Record an epoch with no improvement\n",
    "            if self._best_loss < epoch_loss - early_stop_tol:\n",
    "                self._epochs_no_improvement += 1\n",
    "\n",
    "            # Record an improvement in the loss\n",
    "            if self._best_loss > epoch_loss:\n",
    "                self.best_epoch = epoch\n",
    "                self._best_loss = epoch_loss\n",
    "                self._epochs_no_improvement = 0\n",
    "\n",
    "            # Print epoch info\n",
    "            print(f\"Epoch complete: {epoch+1}/{max_epochs}, Loss: {epoch_loss}, Best loss: {self._best_loss}\")\n",
    "\n",
    "            # Early stop if necessary\n",
    "            if self._epochs_no_improvement >= early_stop:\n",
    "                print(f\"Early stopping after epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Method to update model training data (kernel hyperparameters unchanged, no additional training)\n",
    "    # Returns an ExactGP model\n",
    "    def update_train(self, X_update, y_update):\n",
    "        \n",
    "        # Put tensors on GPU if cuda is enabled\n",
    "        if self.cuda:\n",
    "            X_update = X_update.cuda()\n",
    "            y_update = y_update.cuda()\n",
    "\n",
    "        # Update model training data\n",
    "        self.model = self.model.get_fantasy_model(X_update, y_update)\n",
    "\n",
    "    # Predict method (unbatched)\n",
    "    def predict(self, X_test, cpu = True, fast_preds = False):\n",
    "\n",
    "        # Test data to GPU, if cuda enabled\n",
    "        if self.cuda:\n",
    "            X_test = X_test.cuda()\n",
    "\n",
    "        # Activate eval mode\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        # Make predictions\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var(state = fast_preds):\n",
    "\n",
    "            # Returns the approximate model posterior distribution over functions p(f*|x*, X, y)\n",
    "            # Noise is not yet added to the functions\n",
    "            f_posterior = self.model(X_test)\n",
    "\n",
    "            # Returns the approximate predictive posterior distribution p(y*|x*, X, y)\n",
    "            # Noise is added to the functions\n",
    "            y_posterior = self.likelihood(f_posterior)\n",
    "\n",
    "            # Get posterior predictive mean & prediction intervals\n",
    "            # By default, 2 standard deviations around the mean\n",
    "            y_mean = y_posterior.mean\n",
    "            y_lower, y_upper = y_posterior.confidence_region()\n",
    "\n",
    "        # Return data to CPU if desired\n",
    "        if cpu:\n",
    "            y_mean = y_mean.cpu()\n",
    "            y_lower = y_lower.cpu()\n",
    "            y_upper = y_upper.cpu()\n",
    "\n",
    "        return y_mean, y_lower, y_upper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c21642-37e4-4a0f-a203-f945721ebeec",
   "metadata": {},
   "source": [
    "## Model training & testing without online updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb4625-2c33-422c-8e02-7416583ba835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last N training points to use as inducing points\n",
    "N_inducing_points = int(len(X_train) * 0.4)\n",
    "#N_inducing_points = 24 * 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a3109-e1ca-4b1c-a95d-2e2e1d0f7264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model & likelihood\n",
    "inducing_points = X_train.cuda()\n",
    "#inducing_points = X_train[-N_inducing_points:, :].cuda() \n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPModel(inducing_points)\n",
    "trainer = VariationalMinibatchGP(model, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc1647-243c-4799-9a85-d16bf9ae9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform training\n",
    "trainer.train(\n",
    "    train_loader,\n",
    "    num_data = y_train.size(0), # N. of total observations in training data\n",
    "    max_epochs = 50,\n",
    "    early_stop = 5,\n",
    "    early_stop_tol = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a3b36a-27f8-4139-9d8c-a4de2007a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "preds_mean, preds_upper, preds_lower = trainer.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f129f253-74bb-4d2c-b337-8ef16ae40ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted vs. actual, first 5 days of testing data\n",
    "plt.plot(X_test[:120, 0], y_test[:120])\n",
    "plt.plot(X_test[:120, 0], preds_mean[:120])\n",
    "plt.fill_between(X_test[:120, 0], preds_lower[:120], preds_upper[:120], alpha = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9157ce7-a4ca-4b22-9735-a232410b3e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted vs. actual, entire test set\n",
    "plt.plot(X_test[:, 0], y_test)\n",
    "plt.plot(X_test[:, 0], preds_mean)\n",
    "plt.fill_between(X_test[:, 0], preds_lower, preds_upper, alpha = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a84582-096d-49dc-9ae2-32165b646a80",
   "metadata": {},
   "source": [
    "## Model testing with online updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b064d59-c047-4531-9b3c-f2d2bbbc1df0",
   "metadata": {},
   "source": [
    "Use get_fantasy_model method to update trained model's training data with new input sequences. Hyperparameters are not updated, which kind of mirrors the usage of input sequnces in NN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d0c39-64ec-44f8-8c96-53f1d7ef5e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation pseudocode:\n",
    "Create preds list\n",
    "Perform feature scaling\n",
    "Train until [:first_t]\n",
    "Predict on first_t + horizon\n",
    "Save preds\n",
    "For pred points in [first_t:] // stride:\n",
    "    Perform feature scaling\n",
    "    Expand training set & online train\n",
    "    Predict on first_t + eval index * stride + horizon\n",
    "    Save preds\n",
    "Concat & return preds, actual targets[first_t:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d49a9-6589-4055-86cd-0325490f5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted vs. actual, entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d716194-ebbe-4faf-b5d1-9fafaf4e077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted vs. actual, select sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946400b-88a9-4cb3-8397-186e8f8b6579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
